# -*- coding: utf-8 -*-
"""Endterm_SE-2110_Baiuzak_Amina.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WolICx6wXWVgcHSz3c6zafRZzO7R9uCJ

# Clustering method on urban road accidents using K-Means, Gaussian Mixture Model (GMM), and DBSCAN

# 1. Data Preparation

For this assignment I chose dataset about coordinates (longitude, latitude) of road accidents occurred in urban areas in Great Britain [1].
"""

import pandas as pd
from zipfile import ZipFile
from sklearn.preprocessing import StandardScaler

zip_file_path = 'urbangb+urban+road+accidents+coordinates+labelled+by+the+urban+center.zip'

with ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall()

df = pd.read_csv('urbanGB.txt', header=None, names=['Longitude', 'Latitude'])

print("Dataset:")
print(df.head())

# Data preprocessing
# Reducing the dataset
df = df.sample(n=10000, random_state=42)

# Handling missing values
df.dropna(inplace=True)

# Scaling
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns
scaler = StandardScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

X = df

print("\nPreprocessed Dataset:")
print(X.head())

# Visualizing the results using PCA for dimensionality reduction
def plot(X, title, column_name):
   # Scatter plot for clustering
  plt.scatter(df['Longitude'], df['Latitude'], c=df[column_name], cmap='viridis')
  plt.title(title)
  plt.xlabel('Longitude')
  plt.ylabel('Latitude')
  plt.show()

"""# 2. K-Means Clustering"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score
from yellowbrick.cluster import KElbowVisualizer

# Finding the optimal number of clusters using the Elbow method
model = KMeans()
visualizer = KElbowVisualizer(model, k=(1, 20))
visualizer.fit(X)
visualizer.show()

# Applying K-Means clustering with the optimal K
kmeans = KMeans(n_clusters=4)
df['KMeans_Cluster'] = kmeans.fit_predict(X)

plot(X, 'K-Means Clustering', 'KMeans_Cluster')

# Evaluating clustering using silhouette score
silhouette_avg = silhouette_score(X, df['KMeans_Cluster'])
print(f"Silhouette Score: {silhouette_avg}")

"""# 3. Gaussian Mixture Models (GMM) Clustering"""

from sklearn.mixture import GaussianMixture

# Implementing the Gaussian Mixture Models (GMM) clustering algorithm
gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=42)

# Hyperparameter tuning
best_silhouette_score = -1
best_n_components = -1
best_covariance_type = ''

for n_components in range(2, 11):
    for covariance_type in ['full', 'tied', 'diag', 'spherical']:
        # Creating GMM model with current hyperparameters
        gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=42)
        df['GMM_Cluster'] = gmm.fit_predict(X)

        # Calculating silhouette score
        silhouette_avg = silhouette_score(X, df['GMM_Cluster'])

        print(f"n_components={n_components}, covariance_type={covariance_type}: Silhouette Score = {silhouette_avg}")

        # Updating best hyperparameters if a better silhouette score is obtained
        if silhouette_avg > best_silhouette_score:
            best_silhouette_score = silhouette_avg
            best_n_components = n_components
            best_covariance_type = covariance_type

# Best hyperparameters
print(f"Best Hyperparameters: n_components={best_n_components}, covariance_type={best_covariance_type}, best_silhouette_score={best_silhouette_score}")

# Applying GMM clustering with the best hyperparameters
gmm = GaussianMixture(n_components=best_n_components, covariance_type=best_covariance_type, random_state=42)
df['GMM_Cluster'] = gmm.fit_predict(X)

plot(X, 'GMM Clustering', 'GMM_Cluster')

"""# 4. DBSCAN Clustering"""

from sklearn.cluster import DBSCAN
import numpy as np

# Implementing the DBSCAN clustering algorithm
dbscan = DBSCAN(eps=0.5, min_samples=5)

# Defining a range of values for 'eps' and 'min_samples' to experiment with
eps_values = np.linspace(0.1, 1.0, 10)
min_samples_values = range(3, 11)

best_silhouette_score = -1
best_eps = None
best_min_samples = None

# Iterating over different combinations of hyperparameters
for eps in eps_values:
    for min_samples in min_samples_values:
        # Creating DBSCAN model with current hyperparameters
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        df['DBSCAN_Cluster'] = dbscan.fit_predict(X)

        # Check if there are more than one cluster before computing Silhouette Score
        unique_labels = df['DBSCAN_Cluster'].unique()
        if len(unique_labels) > 1:
            silhouette_avg = silhouette_score(X, df['DBSCAN_Cluster'])
            print(f"Silhouette Score: {silhouette_avg}")
        else:
            # print("DBSCAN resulted in only one cluster, Silhouette Score cannot be computed.")
            continue

        print(f"eps={eps:.2f}, min_samples={min_samples}: Silhouette Score = {silhouette_avg}")

        # Updating best hyperparameters if a better silhouette score is obtained
        if silhouette_avg > best_silhouette_score:
            best_silhouette_score = silhouette_avg
            best_eps = eps
            best_min_samples = min_samples

# Best hyperparameters
print(f"Best Hyperparameters: eps={best_eps:.2f}, min_samples={best_min_samples}, best_silhouette_score={best_silhouette_score}")

# Applying DBSCAN clustering with the best hyperparameters
best_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)
df['DBSCAN_Cluster'] = best_dbscan.fit_predict(X)

plot(X, 'DBSCAN Clustering', 'DBSCAN_Cluster')

"""# 5. Evaluation and Comparison"""

from sklearn.metrics import davies_bouldin_score

def evaluate(labels, name):
  # Davies-Bouldin Index
  davies_bouldin_kmeans = davies_bouldin_score(X, labels)
  print(f"\nDavies-Bouldin Index for {name}: {davies_bouldin_kmeans}")

  # Silhouette Score
  silhouette_kmeans = silhouette_score(X, labels)
  print(f"Silhouette Score for {name}: {silhouette_kmeans}")

# K-Means
kmeans_labels = KMeans(n_clusters=4, random_state=42).fit_predict(X)

# Gaussian Mixture Model
gmm_labels = GaussianMixture(n_components=9, covariance_type='tied', random_state=42).fit_predict(X)

# DBSCAN
dbscan_labels = DBSCAN(eps=0.60, min_samples=3).fit_predict(X)

evaluate(kmeans_labels, 'K-Means')
evaluate(gmm_labels, 'Gaussian Mixture Model')
evaluate(dbscan_labels, 'DBSCAN')

"""**Summarizing: On the given dataset, K-Means, Gaussian Mixture Model (GMM), and DBSCAN were used for clustering analysis. K-Means and GMM performed similarly, demonstrating modest cluster separation and compactness, making them ideal for datasets with roughly spherical clusters. They are, however, sensitive to initial centroids and may struggle with irregularly formed clusters. In terms of well-defined and well-separated clusters, DBSCAN excelled, displaying tolerance to noise and outliers. It might have difficulty with clusters of varying densities, and hyperparameters must be carefully adjusted. Overall, the clustering method should be chosen based on the properties of the data, with K-Means and GMM being appropriate for uniform clusters and DBSCAN excelling in scenarios with unevenly shaped and varied-density clusters.**

References

[1] UrbanGB, urban road accidents coordinates labelled by the urban center (2019) UCI Machine Learning Repository. Available at: https://archive.ics.uci.edu/dataset/550/urbangb+urban+road+accidents+coordinates+labelled+by+the+urban+center
"""